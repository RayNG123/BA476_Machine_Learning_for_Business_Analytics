{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4AI6kCjUdadG"},"source":["# Lab - Regularised regression\n","\n","You explored the computational advertising dataset provided by Cogo Labs as part of the descriptive analytics exercise, and should have some intution about what factors influence email open rates. It is now time to translate this intuition into a machine learning model to predict email open rates for new customers based on their browsing behaviour.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"j_jWf_sDdxk6"},"source":["## Setup\n","\n","Lets start by importing the packages we'll need and mounting our Google Drive as before. "]},{"cell_type":"code","metadata":{"id":"ZNxLcICQuclV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664822557904,"user_tz":240,"elapsed":56944,"user":{"displayName":"Gerdus Benade","userId":"10720838145833134874"}},"outputId":"4fcee7c8-12e2-42dc-ac1f-b5ac7787d4c0"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"D9Xg2nxHd-b7"},"source":["We'll use the `read_csv` function to read the dataset, be sure to take a look at the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html). There are mamy optional arguments you may find useful when working on your project dataset. "]},{"cell_type":"code","metadata":{"id":"xqONUClOu9vo","executionInfo":{"status":"ok","timestamp":1664822562314,"user_tz":240,"elapsed":1058,"user":{"displayName":"Gerdus Benade","userId":"10720838145833134874"}}},"source":["df = pd.read_csv('/content/drive/My Drive/ba476-test/data/cogo-all.tsv', delimiter='\\t')"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0OB4IU45eY4T"},"source":["We have to split the data into a training and testing set. `sklearn.model_selection` offers us automated ways of doing this which we will use in the future but since this is our first time, let's do it manually. \n","\n","We create a new column called `train` which is `True` if the instance should be included in the training by using the numpy random number generator. Once we have this column, we can filter on it to create two new dataframes. "]},{"cell_type":"code","metadata":{"id":"iqYd7CNywwVO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664822568964,"user_tz":240,"elapsed":169,"user":{"displayName":"Gerdus Benade","userId":"10720838145833134874"}},"outputId":"433ccce1-ca9d-45ae-c0b8-7d3db25305cf"},"source":["df['train'] = np.random.rand(len(df)) < 0.8\n","\n","df_train = df[df.train == True]\n","df_test = df[df.train == False]\n","\n","print(df_train.shape, df_test.shape)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["(230788, 18) (57510, 18)\n"]}]},{"cell_type":"markdown","metadata":{"id":"Hg5CwMumfQjJ"},"source":["We can refresh our memories of what goes on in the dataset by looking at the column names. "]},{"cell_type":"code","metadata":{"id":"E3SCMbcXAWTX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664822576425,"user_tz":240,"elapsed":171,"user":{"displayName":"Gerdus Benade","userId":"10720838145833134874"}},"outputId":"89f13faa-258d-40ce-ee16-7f854863f4a8"},"source":["df.columns"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['state', 'user_id', 'browser1', 'browser2', 'browser3', 'device_type1',\n","       'device_type2', 'device_type3', 'device_type4', 'activity_observations',\n","       'activity_days', 'activity_recency', 'activity_locations',\n","       'activity_ids', 'age', 'gender', 'p_open', 'train'],\n","      dtype='object')"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"9s5oNkZ-fHTs"},"source":["## Training a linear regression model"]},{"cell_type":"markdown","metadata":{"id":"ou8rI9j0f5f5"},"source":["Let's start by setting up a very simple model that only cares about which browser a user uses. We will create a list, `predictors1` to hold ne column names of the predictors we want to include in our model to make indexing easier."]},{"cell_type":"code","metadata":{"id":"MHC31vFIgj84","executionInfo":{"status":"ok","timestamp":1664822607490,"user_tz":240,"elapsed":194,"user":{"displayName":"Gerdus Benade","userId":"10720838145833134874"}}},"source":["predictors = ['browser1', 'browser2', 'browser3']\n","X1_train = df_train[predictors]\n","X1_test = df_test[predictors]\n","y_train = df_train['p_open']\n","y_test = df_test['p_open']"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KHaZtj6cgrWi"},"source":["Now we can follow the same four steps as always. First, choose a model class, instantiante the model and set hyperparameter values, then fit to your data. Remember we can access model attributes, in this case the coefficients and intercepts term.  "]},{"cell_type":"code","source":["# 1. choose model class\n","# 2. instantiate model\n","# 3. fit model to data"],"metadata":{"id":"BqyM7uaaMMmG"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZKv-j8ctxqMi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664822611419,"user_tz":240,"elapsed":857,"user":{"displayName":"Gerdus Benade","userId":"10720838145833134874"}},"outputId":"ced16501-0c03-437f-dd9b-fefc722f79f7"},"source":["from sklearn.linear_model import LinearRegression # 1. choose model class\n","model = LinearRegression(fit_intercept=True)      # 2. instantiate model\n","model.fit(X1_train, y_train)                      # 3. fit model to data\n","\n","model.coef_, model.intercept_"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([0.01675152, 0.00675408, 0.04699452]), 0.07030910586403696)"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"JzwefjGYhlyz"},"source":["Finally, we make predictions on the training and test set and evaluate the mean squared error. Of course, there are automated functions for this, but let's do it manually so that we can make sure we understand how it works. \n","\n","First we predict on the training data:"]},{"cell_type":"code","source":["# 4a. predict on training data, evaluate"],"metadata":{"id":"EgvEdZCIMVFg"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iME4cItosYor","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664822627188,"user_tz":240,"elapsed":154,"user":{"displayName":"Gerdus Benade","userId":"10720838145833134874"}},"outputId":"6e3c3ba0-735c-4c01-b2b3-2a12bf41c7b4"},"source":["y_train_fit = model.predict(X1_train)              # 4a. predict on training data\n","\n","mse_train = np.mean( (y_train - y_train_fit)**2 )\n","print(np.sqrt(mse_train), mse_train)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["0.1689604790883558 0.028547643493766713\n"]}]},{"cell_type":"markdown","metadata":{"id":"0czfTAwGs5Wm"},"source":["Then on the testing data:"]},{"cell_type":"code","source":["# 4b. predict on test data, evaluate"],"metadata":{"id":"GYpIWaD6MYDQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5H6Qo8jNtmN1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664822630924,"user_tz":240,"elapsed":197,"user":{"displayName":"Gerdus Benade","userId":"10720838145833134874"}},"outputId":"4cf30227-b6c3-4f5e-902a-24b00b0bacb3"},"source":["y_test_fit = model.predict(X1_test)                # 4b. predict on test data\n","mse_test = np.mean( (y_test - y_test_fit)**2 )\n","print(np.sqrt(mse_test), mse_test)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["0.16900489275438607 0.028562653774921537\n"]}]},{"cell_type":"markdown","metadata":{"id":"Si6Ov0yps8Rl"},"source":["In this case our MSE is pretty similar, so it's unlikely we overfit. Is this a \"good\" MSE? We don't really know yet, but we can say that our open-rate predictions are, on average, off by about 17\\%. \n"]},{"cell_type":"markdown","metadata":{"id":"fc4pLaLStH0C"},"source":["## Ridge regression\n","\n","Recall that ridge regression minimizes $\\sum_i (y_i - \\hat{y}_i) + \\lambda \\sum_{j=1}^p \\beta_j^2, $ where $\\lambda$ is the regularization paramter and $\\beta$ the coefficients of the model. \n","In the `Ridge` model, the regularisation parameter is called `alpha`.\n","\n","Since we are penalising the coefficients, it is important that all the predictors are on the same scale. Here, we'll use the `StandardScalar` transform to ensure every predictor has mean  0 and standard deviation 1. Make sure you look at the [StandardScalar documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). \n","\n","Note that there is a difference between standardisation and normalisation, which simply ensures all values fall between 0 and 1. "]},{"cell_type":"code","metadata":{"id":"t84AcK-W7wgI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664822737495,"user_tz":240,"elapsed":356,"user":{"displayName":"Gerdus Benade","userId":"10720838145833134874"}},"outputId":"750b469d-fb7d-43ff-f7bd-bf3410987e2a"},"source":["from sklearn.linear_model import Ridge\n","from sklearn.preprocessing import StandardScaler \n","\n","scaler = StandardScaler() \n","X1_std_train = scaler.fit_transform(X1_train)\n","X1_std_test = scaler.transform(X1_test)\n","\n","print(X1_std_train[:,:].std(axis=0), X1_std_train[:,:].mean(axis=0))\n","print(X1_std_test[:,:].std(axis=0), X1_std_test[:,:].mean(axis=0))"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[1. 1. 1.] [8.49740000e-18 8.35500697e-17 2.01505373e-17]\n","[0.99609547 0.99986625 0.9934828 ] [-0.00353748 -0.00233341 -0.00313461]\n"]}]},{"cell_type":"markdown","metadata":{"id":"RiTYZGevCUdU"},"source":["Notice that after standadization the test set doesnot perfectly have mean 0 and std 1, because the transformer was fit on the training set. \n","\n","Now we can instantiate and fit the model. After fitting `Ridge.coeff_` contains the coefficients. "]},{"cell_type":"code","metadata":{"id":"t6OS6Jp_ByVQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664822917040,"user_tz":240,"elapsed":387,"user":{"displayName":"Gerdus Benade","userId":"10720838145833134874"}},"outputId":"89b4460c-1c57-4ab3-8174-13b0770243f0"},"source":["#all_alphas = np.logspace(-4, 6, 10)\n","\n","ridge = Ridge(alpha=0.5)   # instantiate\n","ridge.fit(X1_std_train, y_train)            # fit to training data\n","\n","ridge.coef_"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.00563458, 0.00337172, 0.01021803])"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"iEUwPuBqElQM"},"source":["We can compute the MSE manually as before. Let's just do the test MSE this time. "]},{"cell_type":"code","source":["# predict and evalaute on test set"],"metadata":{"id":"cdAAnGb1MtJZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SF2MP56e2QAJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664822919424,"user_tz":240,"elapsed":152,"user":{"displayName":"Gerdus Benade","userId":"10720838145833134874"}},"outputId":"7c508703-39de-40b0-fdea-67565bb18c61"},"source":["y_test_fit_ridge = ridge.predict(X1_std_test)\n","mse_test_ridge = np.mean( (y_test - y_test_fit_ridge)**2)\n","print(np.sqrt(mse_test_ridge), mse_test_ridge)"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["0.16900489269225985 0.02856265375392227\n"]}]},{"cell_type":"markdown","source":["Alternatively, we can do this with a pipeline. Notice that we get the same result. "],"metadata":{"id":"D0EI2Ol3LdcF"}},{"cell_type":"code","source":["from sklearn.pipeline import make_pipeline\n","# create a pipeline equivalent to what was fit before"],"metadata":{"id":"kBO-s7UQMwmh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.pipeline import make_pipeline\n","\n","pipe = make_pipeline(StandardScaler(), Ridge(alpha=0.5))\n","pipe.fit(X1_train, y_train)\n","yhat_test = pipe.predict(X1_test)\n","print(np.mean( (y_test - yhat_test)**2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQpHfTUJK6_V","executionInfo":{"status":"ok","timestamp":1664823297625,"user_tz":240,"elapsed":443,"user":{"displayName":"Gerdus Benade","userId":"10720838145833134874"}},"outputId":"13277817-93f7-4561-d4e9-b56aea022b47"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["0.02856265375392227\n"]}]},{"cell_type":"markdown","metadata":{"id":"g7rpJJHWE8iT"},"source":["## Fitting Lasso and ElasticNet\n","Recall that lasso minimizes $\\sum_i (y_i - \\hat{y}_i) + \\lambda \\sum_{j=1}^p |\\beta_j|, $ where $\\lambda$ is the regularization paramter and $\\beta$ the coefficients of the model.\n","The `Lasso` interface is very similar to `Ridge`. Instantiate the model, fit it to your training data, make predictions and evaluate the test MSE.  \n"]},{"cell_type":"code","metadata":{"id":"oMhehAfhFrTG"},"source":["# import Lasso from sklearn.linear_model\n","\n","# instantiate the Lasso model\n","\n","# fit to X1_std_train\n","\n","# predict on the test set\n","\n","# evaluate MSE on the test set"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yg6qSdVx29VW","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601607981372,"user_tz":240,"elapsed":286,"user":{"displayName":"Johannes Gerhardus Benade","photoUrl":"","userId":"10720838145833134874"}},"outputId":"b260b8be-04a5-4945-e66d-5ffa6b136396"},"source":["from sklearn.linear_model import Lasso\n","lasso = Lasso(alpha=0.001)\n","lasso.fit(X1_std_train, y_train)\n","y_test_fit_lasso = lasso.predict(X1_std_test)\n","mse_test_lasso = np.mean( (y_test - y_test_fit_lasso)**2)\n","print(np.sqrt(mse_test_lasso), mse_test_lasso, lasso.coef_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.16958168059859374 0.028757946394643467 [0.00442118 0.00212429 0.00921021]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FjaiFylqGgt0"},"source":["Elastic nets minimize $\\sum_i (y_i - \\hat{y}_i) + \\lambda \\sum_{j=1}^p (\\gamma |\\beta_j| + (1-\\gamma) \\beta_j^2), $ so the penality term is a linear combination of the ridge and lasso penalty terms. As in `Ridge` and `Lasso` the regularization parameter $\\lambda$ is called `alpha`, and the $\\gamma$ parameter that trades of between the two different penalty terms is called `l1_ratio`. The `l1_ratio` must be between 0 and 1. When `l1_ratio=0`, the penalty is the $\\ell_2$-norm (ridge). \n","\n","When you try multiple $\\gamma$/`l1_ratio` values, it is often a good idea to try several values close to 1 (where the model will behave similar to lasso), for example `[.1, .5, .7, .9, .95, .99, 1]`. \n","\n","Go through the instantiate, fit, predict, evaluate process for `ElasticNet`."]},{"cell_type":"code","metadata":{"id":"cmUI8tDUIoXD"},"source":["# your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rrEjxMso4k38","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601608023664,"user_tz":240,"elapsed":418,"user":{"displayName":"Johannes Gerhardus Benade","photoUrl":"","userId":"10720838145833134874"}},"outputId":"3fad2d58-2652-4149-e7fa-786035b2223d"},"source":["from sklearn.linear_model import ElasticNet\n","elnet = ElasticNet(l1_ratio=0.9)\n","elnet.fit(X1_std_train, y_train)\n","y_test_fit_elnet = elnet.predict(X1_std_test)\n","mse_test_elnet = np.mean( (y_test - y_test_fit_elnet)**2)\n","print(np.sqrt(mse_test_elnet), mse_test_elnet)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.16999396794319874 0.028897949137073282\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SCl_soyWIyqY"},"source":["## Categorical predictors and polynomial basis functions\n","\n","All of the features we used above contained numerical values, but what if this is not the case? Now we need a way to encode categorical values to numbers. This is typically done with one-hot encoding. For example, a column `gender` with three possible values (male, female, other) will be transformed into three binary columns, one for each possible categorical value. We will use the [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) for this.\n","\n","Another common transformation is to use polynomial basis functions. Suppose we have predictors $X_1, X_2$. Instead of fitting the linear model $y=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2$, we may want to fit the (still linear) $y=\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1X_2 + \\beta_4X_1^2 + \\beta_5X_2^2.$ To do this we must add the columns $X_1\\cdot X_2, X_1^2$ and $X_2$ to the data matrix. We will use [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) to add these columns automatically, notice that you can pass the maximum degree when instantiating the transformer. \n"]},{"cell_type":"markdown","metadata":{"id":"ZQT1z5OsL74I"},"source":["The example that follows creates a `ColumnTransformer` to transform to do preprocessing on the five feature columns. The three browser columns are left unchanged and kept (if we did not specify `remainder='passthrough'` they would have been discarded); the `gender` column is transformed to two binary columns, one for each of the genders that appear in the data; and we create quadratic (degree 2 polynomial) features for the  `activity_days` column. "]},{"cell_type":"code","metadata":{"id":"0QB5LKcn7DIh"},"source":["from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","# all predictors \n","numerical_predictors = ['browser1', 'browser2', 'browser3']\n","categorical_predictors = ['gender'] #['gender', 'state']\n","poly_predictors = ['activity_days']\n","all_predictors = numerical_predictors + categorical_predictors + poly_predictors\n","\n","# list of the two transformation we want to do\n","t = [('cat', OneHotEncoder(), categorical_predictors), \n","     ('poly', PolynomialFeatures(2, include_bias=False), poly_predictors)]\n","\n","# instantiate columntransformer with our transforamtions t\n","col_transform = ColumnTransformer(transformers=t, remainder='passthrough')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VMNrf7pZNILE"},"source":["Now we can apply the transformation to our training and testing sets. Notice that we have 7 columns after transformation: 3 for the browsers; 2 for the gender one-hot encoding and 2 for `activity_days` and `activity_days`$^2$."]},{"cell_type":"code","metadata":{"id":"ORi-uzoK-SVA","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601608108472,"user_tz":240,"elapsed":346,"user":{"displayName":"Johannes Gerhardus Benade","photoUrl":"","userId":"10720838145833134874"}},"outputId":"83603721-9dd5-4e07-b97c-33cddbeeacab"},"source":["xt_train = col_transform.fit_transform(df_train[all_predictors])\n","xt_test = col_transform.fit_transform(df_test[all_predictors])\n","xt_train.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(230660, 7)"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"markdown","metadata":{"id":"9Pr3LCkvNzXE"},"source":["We can choose and fit a model as before on this new data. "]},{"cell_type":"code","metadata":{"id":"ODoxuD3IN5qO"},"source":["# your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Beg402dq91Ax","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1601608110471,"user_tz":240,"elapsed":291,"user":{"displayName":"Johannes Gerhardus Benade","photoUrl":"","userId":"10720838145833134874"}},"outputId":"b82ce92d-942e-47e7-b99c-2ae55314e5b3"},"source":["model = LinearRegression(fit_intercept=True)\n","model.fit(xt_train, y_train)\n","yhat_test = model.predict(xt_test)\n","mse_test = np.mean( (y_test - yhat_test)**2)\n","print(mse_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.028583370783762215\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TI_ZThaMN7ie"},"source":["Did the model improve? Experiment some more and see if you can improve the model by adding features, trying different transformations or tuning the regularization parameters. \n","\n","(Hint: When you're stuck, try including `state` in the categorical predictors above.)"]}]}